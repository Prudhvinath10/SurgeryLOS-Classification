# ğŸ¥ Surgery Length of Stay Prediction

**Leveraging Machine Learning to Predict Post-Surgery Length of Stay**

This repository contains our final project for the UT ORIE Summer 2024 Applied Projects. Our goal? Predict how long a patient will stay in the hospital after surgery, based on data available either **before** or **after** surgery. This could help hospitals assign inpatient/outpatient status more intelligentlyâ€”without relying on a stethoscope and a dream.

---

## ğŸ“Œ Objective

Develop robust ML models to predict **Post-Surgery Length of Stay (LOS)** into 4 discrete categories using structured surgical data. Our mission: optimize hospital capacity by forecasting how long patients will hang out in recovery, from **pre-op data** to **post-op insights**.

---

## ğŸ§  What We Did

- ğŸ§½ **Outlier Detection** with flexible IQR filtering (1.0â€“3.0)
- ğŸ¤• **Missing Value Imputation**:
  - *Numerical*: Median by groups like Age Bucket, Sex, Service
  - *Categorical*: Mode fill, 'Unknown' handling
- ğŸ“Š **Feature Engineering & Dropping** of redundant or sparsely populated features
- ğŸ“¦ **One-Hot Encoding** to convert categorical features into 6000+ columns of pure regret
- ğŸ§® **Model Training**:
  - Random Forest Classifier (RFC)
  - XGBoost (XGB)
- ğŸ§ª **Model Evaluation**: Accuracy, Recall, Precision, F1-Score, Cross-Validation
- ğŸ” **Feature Selection**: Forward & Backward Selection to reduce dimensionality
- ğŸ’¡ **Explainability**: SHAP analysis to see what features actually mattered

---

## ğŸ”¬ Results Summary

### Post-Operative Predictions

| Model        | Accuracy | F1 Score | Notes |
|--------------|----------|----------|-------|
| RFC (Base)   | 93.43%   | 93%      | Strong baseline |
| RFC (Tuned)  | 95.24%   | 95%      | With feature selection |
| XGB (Tuned)  | 83.60%   | 84%      | Improved with deeper trees |

### Pre-Operative Predictions

| Model        | Accuracy | F1 Score | Notes |
|--------------|----------|----------|-------|
| RFC (Base)   | 89.39%   | 89%      | Decent from early data |
| RFC (Tuned)  | 93.29%   | 93%      | With feature selection |
| XGB (Tuned)  | 78.64%   | 79%      | Trying its best |

---

## ğŸ§¬ Data Overview

- Total records: 139,634
- Final cleaned post-op dataset: 38,487 rows
- Final cleaned pre-op dataset: 30,409 rows
- Features after one-hot encoding: 5,000â€“6,000+
- Target variable: `LOS_4_Groups`

| Class | Label           |
|-------|-----------------|
| 0     | 60+ hours       |
| 1     | <12 hours       |
| 2     | 12â€“36 hours     |
| 3     | 36â€“60 hours     |

---

## ğŸ““ Notebooks

| File | Description |
|------|-------------|
| `DataCleaning_Graphs.ipynb` | Outlier analysis, missing value imputation, and visualizations of numerical features |
| `RFC.ipynb` | Random Forest Classifier model training and evaluation (Pre-Op and Post-Op); includes feature selection and cross-validation |
| `XGB.ipynb` | XGBoost Classifier with Bayesian optimization; includes SHAP analysis and final model performance evaluation |

---

## ğŸ› ï¸ Tech Stack

- **Python 3.9+**
- `pandas`, `numpy`, `scikit-learn`, `xgboost`
- `matplotlib`, `seaborn`, `shap`
- `bayesian-optimization`
- Jupyter Notebooks

---

## ğŸ’¡ Future Work

- Replace one-hot encoding with **ClinicalBERT** embeddings
- Explore deep learning models or regression for precise LOS predictions
- Investigate SHAP insights further for clinical interpretability
- Analyze misclassification patterns across patient subgroups
- Avoid working with 6,000-column CSVs ever again

---

## ğŸ“ Repository Structure

```text
surgiLOS/
â”œâ”€â”€ data/                      # Raw and processed data (excluded from repo)
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ notebooks/                 # All Jupyter notebooks for the project
â”‚   â”œâ”€â”€ DataCleaning_Graphs.ipynb
â”‚   â”œâ”€â”€ RFC.ipynb
â”‚   â””â”€â”€ XGB.ipynb
â”œâ”€â”€ src/                       # Python scripts for cleaning, modeling, utils (optional)
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ modeling.py
â”‚   â”œâ”€â”€ tuning.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ models/                    # Saved model files (.pkl)
â”œâ”€â”€ reports/                   # Final report and presentation PDFs
â”‚   â”œâ”€â”€ Final_Report.pdf
â”‚   â””â”€â”€ Final_Presentation.pdf
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ README.md                  # This beautiful monstrosity
â””â”€â”€ .gitignore                 # Ignoring sensitive/temporary files
